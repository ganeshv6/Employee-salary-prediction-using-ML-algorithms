# -*- coding: utf-8 -*-
"""Employee salary prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uvpJsPXRvYSEqi92E9osTNKNtNd4L-qp
"""

import pandas as pd

data=pd.read_csv("/content/dataset_employee_updated.csv")

data

data.shape

data.head(10)

data.isna().sum()

print(data.occupation.value_counts())

print(data.education.value_counts())

print(data.workclass.value_counts())

print(data['marital-status'].value_counts())

print(data.gender.value_counts())

print(data.relationship.value_counts())

print(data.race.value_counts())

print(data['native-country'].value_counts())

data.occupation.replace({'?':'others'},inplace=True)

print(data.occupation.value_counts())

data.workclass.replace({'?':'others'},inplace=True)

print(data.workclass.value_counts())

data['native-country'].replace({'?':'others'},inplace=True)

print(data['native-country'].value_counts())

data=data[data['workclass']!='Without-pay']
data=data[data['workclass']!='Never-worked']

print(data.workclass.value_counts())

data=data[data['education']!='Preschool']
data=data[data['education']!='1st-4th']
data=data[data['education']!='5th-6th']

print(data.education.value_counts())

data.shape

data.drop(columns=['education'],inplace=True)

data

import matplotlib.pyplot as plt
plt.boxplot(data['age'])
plt.show()

data=data[(data['age']<=75)&(data['age']>=17)]

plt.boxplot(data['age'])
plt.show()

print(data.workclass.value_counts())

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
data['workclass']=le.fit_transform(data['workclass'])
data['marital-status']=le.fit_transform(data['marital-status'])
data['occupation']=le.fit_transform(data['occupation'])
data['relationship']=le.fit_transform(data['relationship'])
data['race']=le.fit_transform(data['race'])
data['gender']=le.fit_transform(data['gender'])
data['native-country']=le.fit_transform(data['native-country'])
data

x=data.drop(columns=['income'])
y=data['income']

x

y

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
x=scaler.fit_transform(x)
x

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=1, stratify=y)

x_train

from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier()
knn.fit(x_train,y_train)
predict=knn.predict(x_test)
predict

from sklearn.metrics import accuracy_score
accuracy_score(y_test,predict)

from sklearn.linear_model import LogisticRegression
lr=LogisticRegression()
lr.fit(x_train,y_train)
predict1=lr.predict(x_test)
predict1

from sklearn.metrics import accuracy_score
accuracy_score(y_test,predict1)

from sklearn.neural_network import MLPClassifier
clf=MLPClassifier()
clf.fit(x_train,y_train)
predict2=clf.predict(x_test)
predict2

from sklearn.metrics import accuracy_score
accuracy_score(y_test,predict2)

from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Make sure x and y are defined and preprocessed
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

models = {
    "Logistic Regression": LogisticRegression(),
    "Random Forest": RandomForestClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Support Vector Machine": SVC()
}

result = {}

for name, model in models.items():
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('model', model)
    ])
    pipe.fit(x_train, y_train)
    y_pred = pipe.predict(x_test)
    acc = accuracy_score(y_test, y_pred)
    result[name] = acc
    print(f"{name}: Accuracy = {acc}")
    print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
plt.bar(result.keys(),result.values(),color='red')
plt.title('Accuracy of Different Models')
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)
plt.tight_layout()
plt.grid(True)
plt.show()

from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
import pandas as pd
import joblib

# âœ… Select only required columns
features = ['age', 'educational-num', 'occupation', 'hours-per-week', 'experience']
target = 'income'  # Update if your target column has a different name

X = data[features]
y = data[target]

# âœ… Split into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# âœ… Define preprocessing
numeric_features = ['age', 'hours-per-week', 'experience']
categorical_features = ['educational-num', 'occupation']

numeric_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline([
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer([
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])

# âœ… Define models to test
models = {
    "Logistic Regression": LogisticRegression(),
    "Random Forest": RandomForestClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Support Vector Machine": SVC()
}

results = {}
best_pipeline = None
best_model_name = ""
best_accuracy = 0.0

# âœ… Train each model in a pipeline
for name, model in models.items():
    pipe = Pipeline([
        ('preprocessor', preprocessor),
        ('model', model)
    ])
    pipe.fit(x_train, y_train)
    y_pred = pipe.predict(x_test)
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc
    print(f"{name}: Accuracy = {acc:.4f}")

    if acc > best_accuracy:
        best_accuracy = acc
        best_pipeline = pipe
        best_model_name = name

# âœ… Save the best model pipeline
print(f"\nâœ… Best Model: {best_model_name} with Accuracy = {best_accuracy:.4f}")
joblib.dump(best_pipeline, 'best_model.pkl')
print("âœ… Full pipeline saved as 'best_model.pkl'")

!pip install streamlit

!pip install streamlit pyngrok joblib scikit-learn pandas

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import joblib
# import pandas as pd
# 
# # Load the trained pipeline model
# model = joblib.load("best_model.pkl")
# 
# st.set_page_config(page_title="Employee Income Prediction", page_icon="ðŸ’°", layout="wide")
# st.title("ðŸ’¼ Employee Income Prediction")
# st.markdown("Predict whether an employee earns >50K or <=50K based on their details.")
# 
# # Manual label encodings used during training (must match training encoders)
# education_map = {
#     "Bachelors": 0, "HS-grad": 1, "11th": 2, "Masters": 3, "9th": 4,
#     "Some-college": 5, "Assoc-acdm": 6, "7th-8th": 7, "Doctorate": 8,
#     "Prof-school": 9, "Assoc-voc": 10, "10th": 11, "12th": 12
# }
# 
# occupation_map = {
#     "Prof-specialty": 0, "Craft-repair": 1, "Exec-managerial": 2,
#     "Adm-clerical": 3, "Sales": 4, "Other-service": 5,
#     "Machine-op-inspct": 6, "others": 7, "Transport-moving": 8,
#     "Handlers-cleaners": 9, "Farming-fishing": 10, "Tech-support": 11,
#     "Protective-serv": 12, "Priv-house-serv": 13, "Armed-Forces": 14
# }
# 
# # Sidebar inputs
# st.sidebar.header("Enter Employee Details")
# 
# age = st.sidebar.number_input("Age", min_value=18, max_value=70, value=30)
# education = st.sidebar.selectbox("Education Level", list(education_map.keys()))
# occupation = st.sidebar.selectbox("Occupation", list(occupation_map.keys()))
# hours_per_week = st.sidebar.number_input("Hours per Week", min_value=1, max_value=80, value=40)
# experience = st.sidebar.number_input("Years of Experience", min_value=0, max_value=40, value=5)
# 
# # Encode input for prediction
# input_df = pd.DataFrame([{
#     "age": age,
#     "educational-num": education_map[education],
#     "occupation": occupation_map[occupation],
#     "hours-per-week": hours_per_week,
#     "experience": experience
# }])
# 
# 
# st.write("### Input Data Preview")
# st.write(input_df)
# 
# # Predict button
# if st.button("Predict Salary Class"):
#     prediction = model.predict(input_df)[0]
#     st.success(f"ðŸŽ¯ Predicted Salary Class: {'>50K' if prediction == 1 else '<=50K'}")
# 
# # Batch prediction section
# st.markdown("---")
# st.markdown("#### ðŸ“ Batch Prediction (CSV Upload)")
# 
# uploaded_file = st.file_uploader("Upload a CSV file (must use numeric format)", type=["csv"])
# if uploaded_file is not None:
#     batch_data = pd.read_csv(uploaded_file)
#     st.write("ðŸ“‹ Uploaded Data Preview:")
#     st.write(batch_data.head())
# 
#     try:
#         batch_preds = model.predict(batch_data)
#         batch_data['PredictedClass'] = ['>50K' if p == 1 else '<=50K' for p in batch_preds]
#         st.write("âœ… Predictions:")
#         st.write(batch_data)
# 
#         csv = batch_data.to_csv(index=False).encode('utf-8')
#         st.download_button("Download Predictions CSV", csv, "predictions.csv", "text/csv")
#     except Exception as e:
#         st.error(f"âŒ Prediction failed: {e}")
#

!ngrok authtoken 300fkVO8Ril13hAQA5KQqCLokmD_5hSE48fVTXy9eKhksRWpu

from pyngrok import ngrok
import os

# Kill existing streamlit processes (optional but safe)
!pkill streamlit

# Run Streamlit in the background
!nohup streamlit run app.py &>/content/log.txt &

# Wait a few seconds to let Streamlit start (optional)
import time
time.sleep(5)

# Connect ngrok to the local streamlit server
public_url = ngrok.connect("http://localhost:8501")
print(f"Your app is live at: {public_url}")